{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d5fa11",
   "metadata": {},
   "source": [
    "# 2. Базовые методы обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46c8d4",
   "metadata": {},
   "source": [
    "Машинное представление текста на естественном языке;  \n",
    "Предобработка текста: токенизация и сегментация;  \n",
    "Нормализация слов: стеммеры, лемматизаторы;  \n",
    "One Hot Encoding и обратный индекс Тf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b1f8d",
   "metadata": {},
   "source": [
    "### Машинное представление текста на естественном языке "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f4c98",
   "metadata": {},
   "source": [
    "Представление на уровне символов.  \n",
    "\n",
    "Если создать строковую переменную, то в памяти компьютера она будет представлена как набор ячеек памяти, содержащих  единицы и нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1821732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t97\t0b1100001\n",
      "b\t98\t0b1100010\n",
      "c\t99\t0b1100011\n",
      "d\t100\t0b1100100\n",
      "1\t49\t0b110001\n",
      "2\t50\t0b110010\n",
      "3\t51\t0b110011\n",
      "4\t52\t0b110100\n",
      "!\t33\t0b100001\n",
      "@\t64\t0b1000000\n",
      "#\t35\t0b100011\n",
      "$\t36\t0b100100\n"
     ]
    }
   ],
   "source": [
    "example = 'abcd1234!@#$'\n",
    "for s in example:\n",
    "    index = ord(s)\n",
    "    byte = bin(s.encode()[0])\n",
    "    print(s, index, byte, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1584c",
   "metadata": {},
   "source": [
    "Чтобы связать символ с его двоичным представлением, используются кодировки. Про ASCII мы уже упоминали в первой главе. Для кодировки русского текста на ОС Windows часто используют кодировку windows 1251. На ос семейства Linux - utf8. Для кодировки латинских символов требуется один байт на символ, для кириллицы - два байта на символ. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0689fb7",
   "metadata": {},
   "source": [
    "Если при работе с текстом Вам встретятся крякозябры, то знайте - проблема в неправильной кодировке символов. Компьютер при попытке преобразовать единицы и нули в символ использовал не ту таблицу, поэтому получается набор нечитаемых символов. Кодировку можно сменить в продвинутых текстовых редакторах или средствами python (функции encode и decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c713af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет\n"
     ]
    }
   ],
   "source": [
    "print('РџСЂРёРІРµС‚'.encode('cp1251').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020759f1",
   "metadata": {},
   "source": [
    "Представление на уровне слов.  \n",
    "\n",
    "Пусть имеется некоторый достаточно большой корпус текста. Мы можем выписать все уникальные слова из этого корпуса, получив подмножество языка, состоящее из нескольких тысяч слов. Некоторые из этих слов не найти в толковом словаре, но в нашем корпусе они встречаются, и что самое важное, они несут в себе определенный смысл. Воспользуемся заранее подготовленным словарем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3452cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a1', 'a2', 'aa', 'aaa', 'aachen', 'aarhus', 'aaron', 'ab', 'aba'] ... ['zones', 'zoning', 'zoo', 'zoological', 'zoology', 'zoom', 'zu', 'zulu', 'zur', 'zurich']  Всего в словаре 21767 слов.\n"
     ]
    }
   ],
   "source": [
    "# Read file and strip \\n symbols\n",
    "with open('vocab.txt') as f:\n",
    "    vocab = f.readlines()\n",
    "vocab = [s.strip() for s in vocab]\n",
    "\n",
    "# First 10 words\n",
    "print(vocab[:10], '...', vocab[-10:], f' Всего в словаре {len(vocab)} слов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6901fe8",
   "metadata": {},
   "source": [
    "Простейший метод кодирования слова в языке - это его индекс в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c562bd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21516"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03930f",
   "metadata": {},
   "source": [
    "Предложение тоже можно представить как список индексов, для этого его предварительно нужно сегментировать.  \n",
    "\n",
    "**Сегментация текста** (text segmentation) - это процесс разделения текста на значимые единицы, такие как слова, фразы и предложения.  \n",
    "**Токенизация** (tokenize) - частный случай сегментации, в котором разделение основано на четком критерии (обычно по определенному символу).  \n",
    "\n",
    "Например, разделение текста на предложения можно осуществить, используя точку в качестве разделителя. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e12409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the town where I was born', ' Lived a man who sailed to sea']\n"
     ]
    }
   ],
   "source": [
    "text = 'In the town where I was born. Lived a man who sailed to sea'\n",
    "tokens = text.split('.')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f814c",
   "metadata": {},
   "source": [
    "Однако такой алгоритм сломается, если в тексте присутствуют сокращения. На помощь приходят специальные библиотеки, которые реализуют сложные правила и обрабатывают исключения. Мы будем использовать библиотеку nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15face40",
   "metadata": {},
   "source": [
    "Разделение предложения на слова тоже не тривиальная задача, т.к. не во всех естественных языках присутствуют маркеры границ слов, как пробелы в русском и английском. Например, сегментация предложения \"Синхронизация разработки и строительства в Пудуне, Шанхай\" на китайском языке выглядит вот так:  \n",
    "\n",
    "'上海浦东开发与建设同步' → ['上海', '浦东', '开发', ‘与', ’建设', '同步']  \n",
    "\n",
    "К счастью, в этом курсе мы не будем иметь дело с китайским языком, поэтому можем воспользоваться простейшим способом сегментации предложения - токенизация по пробелу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58dc803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'town', 'where', 'i', 'was', 'born']\n"
     ]
    }
   ],
   "source": [
    "sentense = 'In the town where I was born'.lower()\n",
    "tokens = sentense.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d5d3ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9677, 19574, 19938, 21294, 9493, 21119, 2308]\n"
     ]
    }
   ],
   "source": [
    "sent_indexes = [vocab.index(t) for t in tokens]\n",
    "print(sent_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea2346",
   "metadata": {},
   "source": [
    "Что делать со словами, которых нет в словаре?  \n",
    "Простейший способ решения этой проблемы - использование в словаре специального слова \"< UNK >\" (unknown, неизвестный). Тогда если в предложении встретится слово, которого нет в словаре, ему будет присвоен индекс слова < UNK >, например 0.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfed33d",
   "metadata": {},
   "source": [
    "Другой вопрос: слова машина и машины это один и тот же элемент словаря или разные? Насколько большим будет словарь, если для каждой словоформы использовать отдельный код?\n",
    "\n",
    "Для решения этой проблемы используется **нормализация** - замена одного слова на другое (нормальное), которое имеет представление в словаре. В общем случае под нормализацией подразумевается две техники: лемматизация и стеммизация. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539d5a5",
   "metadata": {},
   "source": [
    "**Стемминг** - это простой метод нормализации, чаще всего реализуемый в виде ряда правил, которые постепенно применяются к слову для получения нормализованной формы. Стем - это грубо говоря корень, основа слова. \n",
    "Эти правила варьируются от языка к языку и отражают морфологическую структуру используемого языка. Например, для английского возможным правилом может быть удаление буквы “s” в конце слова во множественном числе, чтобы преобразовать его в единственную форму.  \n",
    "Стемминг в основном используется для индексации документов в поисковой системе, поэтому результатом стемминга могут быть недопустимые слова, например *engine -> engin*. Однако если слово в такой нормальной форме пресутствует в словаре, т.к. обработка идет только внутри для поиска документов и никогда не отображаются пользователю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adc67d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('better')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e5a65",
   "metadata": {},
   "source": [
    "**Лемматизация** — процесс приведения словоформы к лемме — её нормальной (словарной) форме. Грубо говоря, это более сложная версия стемминга. Лемматизация сводит каждое слово к его надлежащей базовой форме, то есть к слову, которое мы можем найти в словаре.  \n",
    "Лемматизация работает точнее, но медленнее, чем стемминг. Продвинутые алгоритмы лемматизации используют контестную информацию для определения части речи слова и правило его приведения к нормальной форме.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e31bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a312614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2acf7",
   "metadata": {},
   "source": [
    "В русском языке нормальными формами считаются следующие морфологические формы: для существительных — именительный падеж, единственное число; для прилагательных — именительный падеж, единственное число, мужской род; для глаголов, причастий, деепричастий — глагол в инфинитиве (неопределённой форме) несовершенного вида. В отличии от английского, для которого уже придумано множество хороших лемматизаторов, улучшение алгоритмов для русского и других, менее распространенных языков, является активной областью ислледований. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c873f",
   "metadata": {},
   "source": [
    "Прямое использование индекса для числового представления слов не самый лучший выбор, потому что числа лучше подходят для описания количества чего-либо. Более правильный и довольно примитивный способ представить слово в виде вектора длиной, равной длине словаря. Все элементы этого вектора равны нулю, за исключением позиции, которой соответствует индекс этого слова. (Рисунок OHE). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4d3cb",
   "metadata": {},
   "source": [
    "Представление на уровне предложений  \n",
    "\n",
    "Как правило, на практике мы имеем дело с предложениями разной длины, что затрудняет их обработку. Мы можем использовать технику One Hot Encoding для кодирования нескольких слов одного предложения или документа. На выходе будет вектор, длина которого равна количеству слов в словаре, а на позициях слов будет количество их вхождений. Т.е. вектор для предложения вычисляется как сумма OHE векторов каждого слова.  \n",
    "\n",
    "Возникает проблема с наиболее часто встречающимися словами (артиклями, предлогами и т.д.), которые не несут смысловой нагрузки и зашумляют OHE вектор. Для ее решения часто используют две техники:\n",
    "- выбрасывают стоп-слова\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d690a5",
   "metadata": {},
   "source": [
    "TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова $t_{i}$ в пределах отдельного документа. \n",
    "\n",
    "$$\\mathrm {tf} (t,d)={\\frac {n_{t}}{\\sum _{k}n_{k}}}$$\n",
    "\n",
    "где $n_t$ есть число вхождений слова $t$ в документ, а в знаменателе — общее число слов в данном документе.  \n",
    "\n",
    "IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Учёт IDF уменьшает вес широкоупотребительных слов. Для каждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF. \n",
    "\n",
    "$$\\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|}}$$\n",
    "где\n",
    "\n",
    "$|D|$ — число документов в коллекции;  \n",
    "$ |\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|$ — число документов из коллекции $D$, в которых встречается $t$ (когда $n_{t}\\neq 0$).  \n",
    "\n",
    "Таким образом, мера TF-IDF является произведением двух сомножителей:\n",
    "\n",
    "$$ \\operatorname {tf-idf}(t,d,D)=\\operatorname {tf}(t,d)\\times \\operatorname {idf}(t,D)$$\n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.  \n",
    "Например, если документ содержит 100 слов, и слово «заяц» встречается в нём 3 раза, то частота слова (TF) для слова «заяц» в документе будет 0,03 (3/100).  \n",
    "Вычислим IDF как десятичный логарифм отношения количества всех документов к количеству документов, содержащих слово «заяц». Таким образом, если «заяц» содержится в 1000 документах из 10 000 000 документов, то IDF будет равной: log(10 000 000/1000) = 4. Для расчета окончательного значения веса слова необходимо TF умножить на IDF. В данном примере, TF-IDF вес для слова «заяц» в выбранном документе будет равен: 0,03 × 4 = 0,12.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d748a3",
   "metadata": {},
   "source": [
    "Предобработка текста: токенизация и нормализация. \n",
    "Библиотека Nltk.\n",
    "Векторное представление текста. Библиотека Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03bc07",
   "metadata": {},
   "source": [
    "Ссылки: \n",
    "- https://nlpub.ru\n",
    "- https://habr.com/ru/company/Voximplant/blog/446738/\n",
    "- Hobson Lane etc, NLP in action, Глава 2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
