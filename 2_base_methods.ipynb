{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d5fa11",
   "metadata": {},
   "source": [
    "# 2. Базовые методы обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46c8d4",
   "metadata": {},
   "source": [
    "1) Машинное представление текста на естественном языке;  \n",
    "2) Предобработка текста: токенизация и сегментация;  \n",
    "3) Нормализация слов: стеммеры, лемматизаторы;  \n",
    "4) One Hot Encoding и обратный индекс Тf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b1f8d",
   "metadata": {},
   "source": [
    "### 1) Машинное представление текста на естественном языке "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f4c98",
   "metadata": {},
   "source": [
    "Представление на уровне символов.  \n",
    "\n",
    "Если создать строковую переменную, то в памяти компьютера она будет представлена как набор ячеек памяти, содержащих  единицы и нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1821732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t97\t0b1100001\n",
      "b\t98\t0b1100010\n",
      "c\t99\t0b1100011\n",
      "d\t100\t0b1100100\n",
      "1\t49\t0b110001\n",
      "2\t50\t0b110010\n",
      "3\t51\t0b110011\n",
      "4\t52\t0b110100\n",
      "!\t33\t0b100001\n",
      "@\t64\t0b1000000\n",
      "#\t35\t0b100011\n",
      "$\t36\t0b100100\n"
     ]
    }
   ],
   "source": [
    "example = 'abcd1234!@#$'\n",
    "for s in example:\n",
    "    index = ord(s)\n",
    "    byte = bin(s.encode()[0])\n",
    "    print(s, index, byte, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1584c",
   "metadata": {},
   "source": [
    "Чтобы связать символ с его двоичным представлением, используются кодировки. Про ASCII мы уже упоминали в первой главе. Для кодировки русского текста на ОС Windows часто используют кодировку windows 1251. На ос семейства Linux - utf8. Для кодировки латинских символов требуется один байт на символ, для кириллицы - два байта на символ. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0689fb7",
   "metadata": {},
   "source": [
    "Если при работе с текстом Вам встретятся крякозябры, то знайте - проблема в неправильной кодировке символов. Компьютер при попытке преобразовать единицы и нули в символ использовал не ту таблицу, поэтому получается набор нечитаемых символов. Кодировку можно сменить в продвинутых текстовых редакторах или средствами python (функции encode и decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c713af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет\n"
     ]
    }
   ],
   "source": [
    "print('РџСЂРёРІРµС‚'.encode('cp1251').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020759f1",
   "metadata": {},
   "source": [
    "Представление на уровне слов.  \n",
    "\n",
    "Пусть имеется некоторый достаточно большой корпус текста. Мы можем выписать все уникальные слова из этого корпуса, получив подмножество языка, состоящее из нескольких тысяч слов. Некоторые из этих слов не найти в толковом словаре, но в нашем корпусе они встречаются, и что самое важное, они несут в себе определенный смысл. Воспользуемся заранее подготовленным словарем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3452cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a1', 'a2', 'aa', 'aaa', 'aachen', 'aarhus', 'aaron', 'ab', 'aba'] ... ['zones', 'zoning', 'zoo', 'zoological', 'zoology', 'zoom', 'zu', 'zulu', 'zur', 'zurich']  Всего в словаре 21767 слов.\n"
     ]
    }
   ],
   "source": [
    "# Read file and strip \\n symbols\n",
    "with open('texts/vocab.txt') as f:\n",
    "    vocab = f.readlines()\n",
    "vocab = [s.strip() for s in vocab]\n",
    "\n",
    "# First 10 words\n",
    "print(vocab[:10], '...', vocab[-10:], f' Всего в словаре {len(vocab)} слов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6901fe8",
   "metadata": {},
   "source": [
    "Простейший метод кодирования слова в языке - это его индекс в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c562bd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21516"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f4468",
   "metadata": {},
   "source": [
    "### 2) Предобработка текста: токенизация и сегментация;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03930f",
   "metadata": {},
   "source": [
    "Предложение тоже можно представить как список индексов, для этого его предварительно нужно сегментировать.  \n",
    "\n",
    "**Сегментация текста** (text segmentation) - это процесс разделения текста на значимые единицы, такие как слова, фразы и предложения.  \n",
    "**Токенизация** (tokenize) - частный случай сегментации, в котором разделение основано на четком критерии (обычно по определенному символу).  \n",
    "\n",
    "Например, разделение текста на предложения можно осуществить, используя точку в качестве разделителя. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e12409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the town where I was born', ' Lived a man who sailed to sea']\n"
     ]
    }
   ],
   "source": [
    "text = 'In the town where I was born. Lived a man who sailed to sea'\n",
    "tokens = text.split('.')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f814c",
   "metadata": {},
   "source": [
    "Однако такой алгоритм сломается, если в тексте присутствуют сокращения. На помощь приходят специальные библиотеки, которые реализуют сложные правила и обрабатывают исключения. Мы будем использовать библиотеку nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15face40",
   "metadata": {},
   "source": [
    "Разделение предложения на слова тоже не тривиальная задача, т.к. не во всех естественных языках присутствуют маркеры границ слов, как пробелы в русском и английском. Например, сегментация предложения \"Синхронизация разработки и строительства в Пудуне, Шанхай\" на китайском языке выглядит вот так:  \n",
    "\n",
    "'上海浦东开发与建设同步' → ['上海', '浦东', '开发', ‘与', ’建设', '同步']  \n",
    "\n",
    "К счастью, в этом курсе мы не будем иметь дело с китайским языком.  \n",
    "Для английского языка простейший способ сегментации предложения на слова - это токенизация по пробелу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58dc803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'town', 'where', 'i', 'was', 'born']\n"
     ]
    }
   ],
   "source": [
    "sentense = 'In the town where I was born'.lower()\n",
    "tokens = sentense.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5d3ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9677, 19574, 19938, 21294, 9493, 21119, 2308]\n"
     ]
    }
   ],
   "source": [
    "sent_indexes = [vocab.index(t) for t in tokens]\n",
    "print(sent_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea2346",
   "metadata": {},
   "source": [
    "Что делать со словами, которых нет в словаре?  \n",
    "Простейший способ решения этой проблемы - использование в словаре специального слова \"< UNK >\" (unknown, неизвестный). Тогда если в предложении встретится слово, которого нет в словаре, ему будет присвоен индекс слова < UNK >, например 0.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdf548",
   "metadata": {},
   "source": [
    "### 3) Нормализация слов: стеммеры, лемматизаторы;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfed33d",
   "metadata": {},
   "source": [
    "В естественных языках одно и то же слово используется в разных падежах и числах. Возникают вопросы: для слова \"машина\" и \"машины\" в единственном и множественном числе использовать одну позицию в словаре или разные? Насколько большим будет словарь, если для каждой словоформы использовать отдельный код?\n",
    "\n",
    "Для решения этой проблемы используется **нормализация** - замена одного слова на другое (нормальное), которое имеет представление в словаре. В общем случае под нормализацией подразумевается две техники: лемматизация и стеммизация. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539d5a5",
   "metadata": {},
   "source": [
    "**Стемминг** - это простой метод нормализации, чаще всего реализуемый в виде ряда правил, которые постепенно применяются к слову для получения нормализованной формы. Стем - это грубо говоря корень, основа слова. \n",
    "Эти правила варьируются от языка к языку и отражают морфологическую структуру используемого языка. Например, для английского языка для преобразование слова в единственную форму возможным правилом может быть удаление буквы “s” в конце.  \n",
    "Стемминг в основном используется для индексации документов в поисковой системе, поэтому результатом стемминга могут быть недопустимые слова, например *engine -> engin*. Такое допускается, если слово в нормальной форме не отображается пользователю и обработка идет только внутри системы, например для поиска документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da07dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc67d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'engin'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('engines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e5a65",
   "metadata": {},
   "source": [
    "**Лемматизация** — процесс приведения словоформы к лемме — её нормальной (словарной) форме. Грубо говоря, это более сложная версия стемминга. Лемматизация сводит каждое слово к его надлежащей базовой форме, то есть к слову, которое мы можем найти в словаре.  \n",
    "Лемматизация работает точнее, но медленнее, чем стемминг. Продвинутые алгоритмы лемматизации используют контестную информацию для определения части речи слова и правило его приведения к нормальной форме.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e31bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a312614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'engine'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('engines', pos='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2acf7",
   "metadata": {},
   "source": [
    "В русском языке нормальными формами считаются следующие морфологические формы: для существительных — именительный падеж, единственное число; для прилагательных — именительный падеж, единственное число, мужской род; для глаголов, причастий, деепричастий — глагол в инфинитиве (неопределённой форме) несовершенного вида. В отличии от английского, для которого уже придумано множество хороших лемматизаторов, улучшение алгоритмов для русского и других, менее распространенных языков, является активной областью ислледований. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44bf47",
   "metadata": {},
   "source": [
    "### 4) One Hot Encoding и обратный индекс Тf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c873f",
   "metadata": {},
   "source": [
    "Использование индекса словаря для представления слов простая, но не самая лучшай техника. По своей природе числа  используются для описания количества чего-либо, а в случае со словами мы имеем дело с разными категориями.  \n",
    "Довольно примитивный, но более подходящий способ представления слов - использование вектора с размером равным длине словаря. Все элементы этого вектора равны нулю, за исключением позиции, которой соответствует индекс этого слова. (Рисунок OHE). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36abae",
   "metadata": {},
   "source": [
    "<img src='imgs/ohe.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8447b67",
   "metadata": {},
   "source": [
    "Представление на уровне предложений  \n",
    "\n",
    "Как правило, на практике мы имеем дело с предложениями разной длины, что затрудняет их обработку. Мы можем использовать технику One Hot Encoding для кодирования нескольких слов одного предложения или документа. На выходе будет вектор, длина которого равна количеству слов в словаре, а на позициях слов будет количество их вхождений. Т.е. вектор для предложения вычисляется как сумма OHE векторов каждого слова.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d77c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.OneHotEncoder(categories=[vocab], handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c3d0b",
   "metadata": {},
   "source": [
    "Т.к. результат содержит очень много нулей и всего одну единицу, то для хранения автоматически используется специальный тип данных - разряженный массив (sparse array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0b95c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_ohe = encoder.fit_transform([['радост'],['гор']])\n",
    "print(sparse_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09b8638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_ohe.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4d3cb",
   "metadata": {},
   "source": [
    "Возникает проблема с наиболее часто встречающимися словами (например, с артиклем the), которые не несут смысловой нагрузки и зашумляют OHE вектор. Для ее решения часто используют две техники:\n",
    "- выбрасывают стоп-слова\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1b19d",
   "metadata": {},
   "source": [
    "Стоп слова - это слова, которые часто встречаются в текстах и используются для связки основных слов. В пакете nltk уже записаны самые распространенные, в т.ч. для русского языка. Нужно только их загрузить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f424117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "ru_stopwords = stopwords.words(\"russian\")\n",
    "print(ru_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93cb4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['дядя', 'самых', 'честных', 'правил', 'шутку', 'занемог']\n"
     ]
    }
   ],
   "source": [
    "sent = 'мой дядя самых честных правил когда не в шутку занемог'\n",
    "filt_sent = [t for t in sent.split() if t not in ru_stopwords]\n",
    "print(filt_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d690a5",
   "metadata": {},
   "source": [
    "TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова $t_{i}$ в пределах отдельного документа. \n",
    "\n",
    "$$\\mathrm {tf} (t,d)={\\frac {n_{t}}{\\sum _{k}n_{k}}}$$\n",
    "\n",
    "где $n_t$ есть число вхождений слова $t$ в документ, а в знаменателе — общее число слов в данном документе.  \n",
    "\n",
    "IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Учёт IDF уменьшает вес широкоупотребительных слов. Для каждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF. \n",
    "\n",
    "$$\\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|}}$$\n",
    "где\n",
    "\n",
    "$|D|$ — число документов в коллекции;  \n",
    "$ |\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|$ — число документов из коллекции $D$, в которых встречается $t$ (когда $n_{t}\\neq 0$).  \n",
    "\n",
    "Таким образом, мера TF-IDF является произведением двух сомножителей:\n",
    "\n",
    "$$ \\operatorname {tf-idf}(t,d,D)=\\operatorname {tf}(t,d)\\times \\operatorname {idf}(t,D)$$\n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.  \n",
    "Например, если документ содержит 100 слов, и слово «заяц» встречается в нём 3 раза, то частота слова (TF) для слова «заяц» в документе будет 0,03 (3/100).  \n",
    "Вычислим IDF как десятичный логарифм отношения количества всех документов к количеству документов, содержащих слово «заяц». Таким образом, если «заяц» содержится в 1000 документах из 10 000 000 документов, то IDF будет равной: log(10 000 000/1000) = 4. Для расчета окончательного значения веса слова необходимо TF умножить на IDF. В данном примере, TF-IDF вес для слова «заяц» в выбранном документе будет равен: 0,03 × 4 = 0,12.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e26414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72cc152a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1aca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.47 0.58 0.38 0.   0.   0.38 0.   0.38]\n",
      " [0.   0.69 0.   0.28 0.   0.54 0.28 0.   0.28]\n",
      " [0.51 0.   0.   0.27 0.51 0.   0.27 0.51 0.27]\n",
      " [0.   0.47 0.58 0.38 0.   0.   0.38 0.   0.38]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d748a3",
   "metadata": {},
   "source": [
    "### Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3201462",
   "metadata": {},
   "source": [
    "Используя два набора положительных и отрицательных слов, создадим конвейр для определения тональности сообщения и интегрируем его с чат ботом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a462969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e9941a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('texts/positive.txt') as f:\n",
    "    pos_words = f.readlines()\n",
    "pos_words = [w.strip() for w in pos_words]\n",
    "pos_words = [stemmer.stem(w) for w in pos_words] \n",
    "\n",
    "with open('texts/negative.txt') as f:\n",
    "    neg_words = f.readlines()\n",
    "neg_words = [w.strip() for w in neg_words]\n",
    "neg_words = [stemmer.stem(w) for w in neg_words] \n",
    "\n",
    "vocab = pos_words + neg_words\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba9bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "msg = 'привет мой хороший чат бот'\n",
    "tokens = word_tokenize(msg, language='russian')\n",
    "filtered_tokens = [t for t in tokens if t not in ru_stopwords]\n",
    "stem_tokens = [[stemmer.stem(t)] for t in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0262482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['привет'], ['хорош'], ['чат'], ['бот']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d404b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = preprocessing.OneHotEncoder(categories=[vocab], handle_unknown='ignore')\n",
    "\n",
    "sparse_vec = encoder.fit_transform(stem_tokens)\n",
    "sent_vec = sparse_vec.sum(axis=0)\n",
    "sent_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0604c",
   "metadata": {},
   "source": [
    "Определим вектора для положительных и отрицательных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "876e3371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_vec = np.zeros((1, len(vocab)))\n",
    "positive_vec[:, :len(pos_words)] = 1\n",
    "negative_vec = np.zeros((1, len(vocab)))\n",
    "negative_vec[:, len(pos_words):] = 1\n",
    "\n",
    "positive_vec, negative_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e3a20",
   "metadata": {},
   "source": [
    "[Скалярное произведение](https://ru.wikipedia.org/wiki/скалярное_произведение) вычисляется как сумма поэлементных произведений двух векторов и в библиотеке numpy имеет свое специальное обозначение - @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08b08371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec @ positive_vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "998b5a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec @ negative_vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eef78319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(msg):\n",
    "    tokens = word_tokenize(msg, language='russian')\n",
    "    filtered_tokens = [t for t in tokens if t not in ru_stopwords]\n",
    "    stem_tokens = [[stemmer.stem(t)] for t in filtered_tokens]\n",
    "    sparse_vec = encoder.fit_transform(stem_tokens)\n",
    "    msg_vec = sparse_vec.sum(axis=0)\n",
    "    pos_score = msg_vec @ positive_vec.T\n",
    "    neg_score = msg_vec @ negative_vec.T\n",
    "    print(pos_score, neg_score)\n",
    "    if pos_score > neg_score:\n",
    "        return 'Это хорошо'\n",
    "    elif neg_score > pos_score:\n",
    "        return 'Это плохо'\n",
    "    else:\n",
    "        return 'Это нормально'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee7625",
   "metadata": {},
   "source": [
    "Протестируем нашу функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5af577e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]] [[2.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Это плохо'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer('голод и горечь')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb2ea834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]] [[0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Это хорошо'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer('весна и веселье')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a72f04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]] [[1.]]\n"
     ]
    }
   ],
   "source": [
    "import telebot\n",
    "\n",
    "TOKEN = '5441620452:AAGQQtoC8ohgdsvsOUO7ubSD6y86oRQ-hL0'\n",
    "\n",
    "bot = telebot.TeleBot(TOKEN) \n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def send_echo(message):\n",
    "    answer = get_answer(message.text)\n",
    "    \n",
    "    bot.send_message(message.chat.id, answer)\n",
    "\n",
    "bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03bc07",
   "metadata": {},
   "source": [
    "Ссылки: \n",
    "- https://nlpub.ru\n",
    "- https://habr.com/ru/company/Voximplant/blog/446738/\n",
    "- Hobson Lane etc, NLP in action, Глава 2. \n",
    "- https://wordsonline.ru/samples/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
