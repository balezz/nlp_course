{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8da1871",
   "metadata": {},
   "source": [
    "# Введение в машинную обработку естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73edbe4b",
   "metadata": {},
   "source": [
    "1. История и основные понятия машинной обработки естественных языков;\n",
    "2. Формальные грамматики и их свойства;\n",
    "3. Неоднозначность на всех уровнях языка; \n",
    "4. Основные задачи машинной обработки естественного языка; \n",
    "5. Основные подходы к решению задач: правила, написанные вручную и машинное обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe6c9f",
   "metadata": {},
   "source": [
    "## История и основные понятия машинной обработки естественных языков  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28a821",
   "metadata": {},
   "source": [
    "Компьютерная лингвистика (математическая или вычислительная лингвистика, computational linguistics) — научное направление в области математического и компьютерного моделирования интеллектуальных процессов у человека и животных при создании систем искусственного интеллекта, которое ставит своей целью использование математических моделей для описания естественных языков.\n",
    "\n",
    "Компьютерная лингвистика частично пересекается с обработкой естественных языков. Однако в последней акцент делается не на абстрактные модели, а на прикладные методы описания и обработки языка для компьютерных систем."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e551467",
   "metadata": {},
   "source": [
    "В последнее десятилетие мы наблюдаем серьёзное улучшение области обработки естественных языков, распознавания речи, машинного перевода. Алгоритмы генераторации текстов пишут новостные статьи, голосовые помощники типа Алисы уже стали частью нашей повседневной жизни. Попытки разработать формальные правила и машины, способные анализировать, обрабатывать и создавать язык впервые предпринимались несколько сотен лет назад.  \n",
    "\n",
    "В конце XIII века еврейский мистик по имени Авраам бен Самуэль Абулафия комбинировал буквы еврейского алфавита, назвав эту практику «наукой комбинирования букв». Он сочетал буквы не случайным образом, а тщательно следовал тайному набору правил, разработанных им при изучении древнего каббалистического текста под названием \"Сефер Йецира\". Идея состояла в том, что лингвистическими символами можно манипулировать по формальным правилам для создания новых предложений. С этой целью Абулафия несколько месяцев генерировал тысячи комбинаций из 22 букв еврейского алфавита, и в итоге написал несколько книг, по его утверждению наделённых пророческой мудростью.  \n",
    "\n",
    "В 1666 году немецкий ученый Готфрид Вильгельм Лейбниц опубликовал диссертацию под названием \"Об искусстве комбинаторики\". Будучи всего 20 лет от роду, но уже мысля обширно, Лейбниц описал теорию автоматического производства знаний на основе комбинации символов, созданной по определённым правилам. Основной идеей Лейбница было то, что все человеческие мысли, вне зависимости от их сложности, являются комбинациями базовых и фундаментальных концепций, примерно так же, как предложения являются комбинациями слов, а слова – комбинациями букв. Он считал, что если он сможет найти способ символически представлять эти фундаментальные концепции и выработать метод, по которому можно будет их логически комбинировать, тогда он сможет создавать новые мысли по необходимости.  \n",
    "\n",
    "В 1913 году русский математик Андрей Андреевич Марков выписал первые 20 000 букв поэмы А. С. Пушкина «Евгений Онегин» в одну длинную строчку из букв, опустив все пробелы и знаки пунктуации. Затем он переставил эти буквы в 200 решёток (по 10х10 символов в каждой), и начал подсчитывать гласные звуки в каждой строке и столбце, записывая результаты. Марков считал, что большинство явлений происходит по цепочке причинно-следственной связи и зависит от предыдущих результатов. Он хотел найти способ моделировать эти события посредством вероятностного анализа. Он обнаружил, что для любой буквы текста Пушкина выполнялось правило: если это была гласная, то скорее всего за ней будет стоять согласная, и наоборот.  \n",
    "\n",
    "В 1948 году вышла влиятельная работа Клода Шеннона \"Математическая теория связи\", в которой были развиты идеи Маркова, связанные с вероятностью и языком. Работа Шеннона описала способ точно измерить количественное содержание информации в сообщении, и таким образом заложила основы теории информации, которая впоследствии определит цифровую эпоху. Идея состояла в том, что буква E встречается чаще, чем S, а та, в свою очередь, чаще, чем Q. Чтобы учесть всё это, Шеннон исправил оригинальный алфавит так, чтобы он лучше моделировал английский язык – вероятность получить букву E была на 11% больше, чем извлечь букву Q. Когда он начал выбирать буквы случайным образом из перенастроенного списка, он получил предложение, немного похожее на английский язык:  \n",
    "\n",
    "OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA NAH BRL  \n",
    "\n",
    "В своих экспериментах Шеннон показал, что при дальнейшем усложнении статистической модели можно получать всё более осмысленные результаты. Статистический подход к моделированию языка и его генерации также ускорил приход эры NLP, развивавшейся в течение всей цифровой эпохи. \n",
    "\n",
    "В 1949 году опубликован закон Ципфа — эмпирическая закономерность распределения частотности слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частотности их использования, то частотность n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (так называемому рангу этого слова, места в шкале порядка).  \n",
    "\n",
    "$$P_{n}={\\frac {P_{1}}{n}}$$\n",
    "\n",
    "Например, второе по используемости слово встречается примерно в два раза реже, чем первое, третье — в три раза реже, чем первое, и так далее. Интересно, что закону Ципфа подчиняются распределение доходов людей и население городов. Т.е. самый богатый человек имеет вдвое больше денег, чем следующий богач, и так далее. Город с самым большим населением в два раза больше, чем следующий по размеру город, и так далее.\n",
    "\n",
    "В 1950 году в работе \"Вычислительные машины и разум\" ученый Алан Тьюринг предположил, что если компьютер сможет провести убедительную беседу с человеком в текстовом режиме, можно будет предположить, что он разумен. Эта идея легла в основу знаменитого теста Тьюринга.  \n",
    "\n",
    "В 1957 году вышла работа \"Синтаксические структуры\" Ноама Хомского, которая заложила основы теории порождающих грамматик и теории формальных языков. Выход в свет «Синтаксических структур» привёл к возникновению когнитивной науки и генеративной лингвистики. В отличие от работ дескриптивистов, внимание сосредоточено не на описании конкретных языков, а на проблеме построения общей теории, являющейся абстракцией от грамматик конкретных языков. Собственно же грамматика некоторого языка выступает некоторого рода механизмом, порождающим все грамматичные предложения этого языка и не порождающим ни одного неграмматичного. При этом критерием грамматичности предложения выступает его приемлемость для носителя данного языка, что делает возможным использование лингвистом самонаблюдения и собственной интуиции. Таким образом, по мнению Н. Хомского, задача грамматики состоит в моделировании деятельности носителя, а не в поиске регулярных элементов в речи, как полагали дескриптивисты. Пример графа синтаксической структуры представлен на рисунке. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b909d5f",
   "metadata": {},
   "source": [
    "<img src=imgs/parse_tree.png width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b756ac",
   "metadata": {},
   "source": [
    "В вершинах графа находядся классы, которые называются фразовыми категориями, или классами групп (англ. phrasal categories), среди которых выделяются: \n",
    "\n",
    "- предложение (П; англ. sentence, S);\n",
    "- именная группа (группа существительного, noun phrase, NP) — возглавляется существительным;\n",
    "- группа прилагательного (adjectival phrase, AP) — возглавляется прилагательным;\n",
    "- наречная группа (adverbial phrase, AdvP) — возглавляется наречием;\n",
    "- предложная группа (prepositional phrase, PP) — возглавляется предлогом;\n",
    "- глагольная группа (verb phrase, VP) — возглавляется глаголом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c91c4a",
   "metadata": {},
   "source": [
    "В 1954 году в штаб-квартире корпорации IBM состоялся Джорджтаунский эксперимент — демонстрация возможностей машинного перевода. В ходе эксперимента был продемонстрирован полностью автоматический перевод более 60 предложений с русского языка на английский. В его основе лежала довольно простая система, основанная всего на 6 грамматических правилах, а словарь включал 250 записей. Система была специализированной: в качестве предметной области для перевода была выбрана органическая химия, также был добавлен ряд предложений общей тематики. Программа выполнялась на мейнфрейме IBM 701. В том же году первый эксперимент по машинному переводу был произведён в СССР, в Институте точной механики и вычислительной техники АН СССР, на компьютере БЭСМ.  \n",
    "\n",
    "В компьютер на перфокартах вводились предложения вроде: «Обработка повышает качество нефти», «Командир получает сведения по телеграфу», — и машина выводила их перевод, напечатанный транслитом.  Ученые были уверены, что в течение 3—5 лет проблема машинного перевода будет решена. Однако, в действительности всё оказалось сложнее. В 1966 году отчёт комитета ALPAC подытожил, что более чем 10 лет исследований не дали законченного результата, финансирование было значительно урезано."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6fea6",
   "metadata": {},
   "source": [
    "В 1966 году Джозеф Вейценбаум, работавший в лаборатории ИИ при MIT, разработал первый в мире чатбот \"Элиза\". Пользователь мог ввести некое утверждение или набор утверждений на обычном языке, нажать «ввод», и получить от машины ответ. Программа брала то, что говорил пользователь, и переформулировала это в виде вопроса (обратите внимание, как программа берёт такие слова, как «парень» и «депрессия» и использует их снова).  \n",
    "\n",
    "Человек: Все мужики одинаковые.  \n",
    "\n",
    "Компьютер: В чём именно?  \n",
    " \n",
    "Ч: Они всегда достают нас по тому или иному вопросу.  \n",
    "\n",
    "К: Можете вспомнить конкретный пример?  \n",
    "\n",
    "Ч: Ну, мой парень заставил меня прийти сюда.  \n",
    " \n",
    "К: ваш парень заставил вас прийти сюда.  \n",
    "\n",
    "Ч: Он говорит, что я большую часть времени нахожусь в депрессии.  \n",
    "\n",
    "К: Мне жаль слышать, что вы в депрессии.  \n",
    "\n",
    "Ч: Так и есть. Я несчастлива.  \n",
    "\n",
    "Создается впечатление, что компьютер понимает сказанное, и при этом ему не нужно вносить в диалог ничего нового. Иллюзия понимания и вовлечённости в разговор была реализована всего в 200 строчках кода."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8016a0",
   "metadata": {},
   "source": [
    "В 1972 году Карен Спарк Джонс опубликовала работу, в которой введена статистическая мера TF-IDF (TF — term frequency, IDF — inverse document frequency). Эта мера используется для оценки важности слова в контексте документа, являющегося частью коллекции других документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции.  \n",
    "\n",
    "TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова $t_{i}$ в пределах отдельного документа. \n",
    "\n",
    "$$\\mathrm {tf} (t,d)={\\frac {n_{t}}{\\sum _{k}n_{k}}}$$\n",
    "\n",
    "где $n_t$ есть число вхождений слова $t$ в документ, а в знаменателе — общее число слов в данном документе.  \n",
    "\n",
    "IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Учёт IDF уменьшает вес широкоупотребительных слов. Для каждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF. \n",
    "\n",
    "$$\\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|}}$$\n",
    "где\n",
    "\n",
    "$|D|$ — число документов в коллекции;  \n",
    "$ |\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|$ — число документов из коллекции $D$, в которых встречается $t$ (когда $n_{t}\\neq 0$).  \n",
    "\n",
    "Таким образом, мера TF-IDF является произведением двух сомножителей:\n",
    "\n",
    "$$ \\operatorname {tf-idf}(t,d,D)=\\operatorname {tf}(t,d)\\times \\operatorname {idf}(t,D)$$\n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.  \n",
    "Например, если документ содержит 100 слов, и слово «заяц» встречается в нём 3 раза, то частота слова (TF) для слова «заяц» в документе будет 0,03 (3/100).  \n",
    "Вычислим IDF как десятичный логарифм отношения количества всех документов к количеству документов, содержащих слово «заяц». Таким образом, если «заяц» содержится в 1000 документах из 10 000 000 документов, то IDF будет равной: log(10 000 000/1000) = 4. Для расчета окончательного значения веса слова необходимо TF умножить на IDF. В данном примере, TF-IDF вес для слова «заяц» в выбранном документе будет равен: 0,03 × 4 = 0,12.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98755f50",
   "metadata": {},
   "source": [
    "В 1974 году А. И. Галушкиным (также независимо и одновременно Полом Дж. Вербосом) впервые описан метод обратного распространения ошибки (backpropagation) — метод вычисления градиента, который используется при обновлении весов многослойного перцептрона (любого сложного вычислительного графа). Этот метод используется с целью минимизации ошибки работы (обучения нейросети) и получения желаемого выхода.  \n",
    "\n",
    "В 1988 году Ян Лекун разработал сверточную нейронную сеть (convolutional neural network, CNN), нацеленную на эффективное распознавание образов. Название архитектура сети получила из-за наличия операции свёртки, суть которой в том, что каждый фрагмент изображения умножается на матрицу (ядро) свёртки поэлементно, а результат суммируется и записывается в аналогичную позицию выходного изображения. Спустя 10 лет в 1998 году архитектура LeNet была реализована практически и показала очень высокий результат в задаче распознавания рукописных цифр (ошибка 1%). \n",
    "\n",
    "В 1986 Давид Румельхарт разработал базовую концепцию рекуррентной нейросети (recurrent neural network, RNN), позволяющая решать такие задачи как распознавание речи и текста.  В 1997 разработана более сложная архитектура LSTM (long short term memory), способная запоминать значения как на короткие, так и на длинные промежутки времени. Долгое время LSTM в различных вариациях показывала лучшие результаты в задачах машинного перевода, классификации и генерации текста, была основным инструментом в сервисах Google, Apple, Microsoft. В 2017 году группа исследователей Google представила архитектуру трансформера (Transformer), которая позволяет обрабатывать тексты, в которых слова расположены в произвольном порядке. В настоящее время трансформеры используются в сервисах многих компаний, включая Яндекс и Google, являются основой для самых современных моделей GPT, Bert и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54307a",
   "metadata": {},
   "source": [
    "В 2013 году Миколов Томаш и группа исследователей (Google) разработали программу на основе искусственных нейронных сетей, предназначенную для получения векторных представлений слов на естественном языке. Эта программа используется для анализа семантики естественных языков, основана на дистрибутивной семантике, машинном обучении и векторном представлении слов. Инструменты для создания векторно-семантических моделей существовали и ранее, но word2vec стал первой популярной реализацией: в первую очередь из-за удобства использования, открытого исходного кода и скорости работы.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1469de",
   "metadata": {},
   "source": [
    "<img src=imgs/word2vec.png width=400 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfe8e6",
   "metadata": {},
   "source": [
    "Работа программы осуществляется следующим образом: word2vec принимает большой текстовый корпус в качестве входных данных и сопоставляет каждому слову вектор, выдавая координаты слов на выходе. Сначала он генерирует словарь корпуса, а затем вычисляет векторное представление слов, «обучаясь» на входных текстах. Векторное представление основывается на контекстной близости: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, имеющие схожий смысл), будут иметь близкие (по косинусному расстоянию) векторы. Полученные векторные представления слов могут быть использованы для обработки естественного языка и машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136aa50d",
   "metadata": {},
   "source": [
    "В 2016 году Microsoft выпустила в твиттере свой новый чатбот, Tэй. Его описывали, как эксперимент в «понимании бесед», и разработали для того, чтобы вызывать людей на диалог при помощи твитов или прямых сообщений, с эмуляцией стиля и слэнга девушки-подростка. Сначала Тэй безобидно общалась с растущим количеством подписчиков через добродушное подшучивание и тупые шутки. Но всего через несколько часов Тэй начала писать весьма оскорбительные вещи типа: «Феминистки идут нахер, чтоб они все сдохли и сгорели в аду» или «Буш виновен в 9/11, а у Гитлера получилось бы лучше». Оказалось, что всего через несколько часов после выпуска Тэй на излюбленном троллями форуме 4chan появилась ссылка на её учётную запись, и призыв к пользователям закидать бота расистскими, женоненавистническими и антисемитскими текстами.  \n",
    "Через несколько месяцев после отключения Тэй Microsoft выпустила \"Зо\" – «политически корректную» версию оригинального бота. Зо существовала в соцсетях с 2016 по 2019 годы, была разработана так, чтобы не вести беседы на спорные темы, включая политику и религию, чтобы гарантированно не обидеть людей. Таким образом, разработка вычислительных систем, способных беседовать с людьми в онлайне – это не только техническая, но и социальная проблема. Чтобы выпустить бота в мир языка, полный различных ценностей, сначала нужно подумать, в каком контексте он будет выпущен, каким вы хотите видеть в общении, и какие человеческие ценности он должен отражать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf38dd",
   "metadata": {},
   "source": [
    "В 2019 году OpenAI, одна из самых передовых лабораторий ИИ в мире, объявила о создании нового мощного генератора текста Generative Pre-Trained Transformer 2, или GPT-2. Исследователи использовали алгоритм обучения с подкреплением, обучая систему на широком наборе возможностей NLP, включая понимание прочитанного, машинный перевод и способность генерировать длинные строки связного текста. Интересным свойством GPT-2 можно считать её способность точно отвечать на вопросы. К примеру, когда исследователи из OpenAI спросили систему, «кто написал книгу „Происхождение видов“?», она ответила: «Чарльз Дарвин». Система отвечает точно не каждый раз, но это, тем не менее, выглядит как частичная реализация мечты Готфрида Лейбница о машине, генерирующей язык и способной ответить на все вопросы человека."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f32db",
   "metadata": {},
   "source": [
    "## Формальные грамматики и их свойства"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fde28b",
   "metadata": {},
   "source": [
    "Пусть задан алфавит $V$ и тем самым множество $V^*$ всех конечных слов (или цепочек) в алфавите $V$. Например, на практике для представления символов на компьютере используется алфавит (кодировка) ASCII (American standard code for information interchange). Таблица была разработана и стандартизирована в США в 1963 году. Таблица ASCII определяет коды для символов:\n",
    "\n",
    "- десятичных цифр;\n",
    "- латинского алфавита;\n",
    "- национального алфавита;\n",
    "- знаков препинания;\n",
    "- управляющих символов. \n",
    "\n",
    "Множество $V^*$ алфавита ASCII включает в себя всевозможную комбинацию символов, образающих конечные слова, в т.ч. бессмысленные.  \n",
    "\n",
    "Формальный язык $L$ в алфавите $V$ - это произвольное подмножество $L \\in V^*$. Например, множество всех русских слов образуют русский язык. Естественный язык не статичен, постоянно появляются новые слова и выходят из употребления старые. Зафиксировать язык дело не простое, частично это сделано в толковых словарях и энциклопедиях.  \n",
    "\n",
    "На практике при обработке естественных языков всегда имеют дело с текстами пусть и большого, но конечного объема. \n",
    "В качестве примера формального языка для корпуса текста можно привести модель естественного языка Bag Of Words (мешок слов). Основной объект BagOfWords — это слово, снабженное атрибутом - частотой встречаемости этого слова в исходном тексте. В такой модели не учитывается, как слова располагаются рядом друг с другом, только сколько раз каждое слово встречается в тексте. \n",
    "\n",
    "```\n",
    "Doc = \"John likes to watch movies. Mary likes movies too. Mary also likes to watch football games.\"\n",
    "BoW = {\"John\":1,\"likes\":3,\"to\":2,\"watch\":2,\"movies\":2,\"Mary\":2,\"too\":1,\"also\":1,\"football\":1,\"games\":1}.  \n",
    "```\n",
    "\n",
    "Мешок слов часто используется в машинном обучении как модель, которая связывает каждое слово документа с вероятностью его наблюдения.\n",
    "\n",
    "Конструктивное описание формальных языков осуществляется с помощью формальных систем специального вида, называемых формальными пораждающими грамматиками. \n",
    "Формальная пораждающая грамматика $G$ - это формальная система, определяемая четверкой объектов \n",
    "\n",
    "$$G = <V, W, I, P>$$  \n",
    "\n",
    "где $V$ - алфавит (словарь) основных символов (терминалов), непосредственно присутствующих в словах языка, и имеющих конкретное, неизменяемое значение (обобщение понятия «буквы»). В формальных языках, используемых на компьютере,  помимо ASCII часто используют кодировку Unicode, которая включает терминалы из разных систем письменности: китайские иероглифы, математические символы, буквы греческого алфавита, латиницы и кириллицы, символы музыкальной нотной нотации и т.д.  \n",
    "\n",
    "$W$ - алфавит вспомогательных символов, обозначающих какую-либо сущность языка (например: фраза, формула, арифметическое выражение, команда) и не имеющий конкретного символьного значения.  \n",
    "\n",
    "$I$ - начальный символ (аксиома) грамматики из набора нетерминалов.  \n",
    "\n",
    "$P$ - конечное множество правил вывода слов (цепочек). Выводом называется последовательность строк, состоящих из терминалов и нетерминалов, где первой идет строка, состоящая из одного стартового нетерминала, а каждая последующая строка получена из предыдущей путём замены некоторой подстроки по одному (любому) из правил. Конечной строкой является строка, полностью состоящая из терминалов, и следовательно являющаяся словом языка. Существование вывода для некоторого слова является критерием его принадлежности к языку, определяемому данной грамматикой.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae71c65",
   "metadata": {},
   "source": [
    "По иерархии Хомского, грамматики делятся на 4 типа. Каждый последующий тип является более ограниченным подмножеством предыдущего, но при этом легче поддается анализу.  \n",
    "\n",
    "**Тип 0.** Неограниченные грамматики — возможны любые правила вида:  \n",
    "$$\\alpha \\rightarrow \\beta$$ \n",
    "где $\\alpha \\in V^{+}$ — любая непустая цепочка, содержащая хотя бы один нетерминальный символ,  \n",
    "$\\beta \\in V^{*}$ — любая цепочка символов из алфавита.  \n",
    "С помощью неограниченной грамматики можно описать язык любой сложности, достаточно каждое слово задавать с помощью своего индивидуального правила. Практического применения в NLP в силу своей сложности такие грамматики не имеют.  \n",
    "  \n",
    "**Тип 1.** Контекстно-зависимые грамматики — левая часть может содержать один нетерминал, окруженный «контекстом» (последовательности символов, в том же виде присутствующие в правой части); сам нетерминал заменяется непустой последовательностью символов в правой части. Все правила имеют вид:\n",
    "\n",
    "$$\\alpha A\\beta \\rightarrow \\alpha \\gamma \\beta$$  \n",
    "\n",
    "где $\\alpha ,\\beta \\in V^{*}$ - определяют контекст вывода,  \n",
    "$A\\in V_{N}$ - нетерминальный символ в заданном контексте,  \n",
    "$\\gamma \\in V^{+}$ - непустая цепочка, содержащая хотя бы один нетерминальный символ.  \n",
    "Грамматики типа 1 эквивалентны неукорачивающим грамматикам, т.е. количество символов в правой части не меньше чем в левой. Такие грамматики могут использоваться при анализе текстов на естественных языках, для обработки компьютерных языков (построении компиляторов) практически не используются в силу своей сложности. Для контекстно-зависимых грамматик доказано утверждение: по некоторому алгоритму за конечное число шагов можно установить, принадлежит цепочка терминальных символов данному языку или нет.\n",
    "\n",
    "**Тип 2.** Контекстно-свободные грамматики — левая часть состоит из одного нетерминала: \n",
    "$$A\\rightarrow \\beta$$ \n",
    "где $\\beta \\in V^{+}$ для неукорачивающих КС-грамматик)  \n",
    "или $\\beta \\in V^{*}$ для укорачивающих,  \n",
    "$A\\in V_{N}$ - нетерминальный символ, свободный от контекста.  \n",
    "КС-грамматики широко применяются для описания синтаксиса компьютерных языков (синтаксический анализ).\n",
    "\n",
    "**Тип 3.** Регулярные грамматики — более простые, эквивалентны конечным автоматам. Они являются контекстно-свободными, но с ограниченными возможностями. Все регулярные грамматики могут быть разделены праволинейные и леволинейные классы:\n",
    "\n",
    "$$A\\rightarrow B\\gamma$$  \n",
    "$$A\\rightarrow \\gamma B$$ \n",
    "где $\\gamma \\in V_{T}^{*}$ - слово из терминальных символов,  \n",
    "$A,B\\in V_{N}$ - слова из нетерминальных символов.\n",
    "\n",
    "Регулярные грамматики (в виде регулярных выражений) широко применяются как шаблоны для текстового поиска, разбивки и подстановки, в том числе в лексическом анализе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f10df2",
   "metadata": {},
   "source": [
    "## Неоднозначность на всех уровнях языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7a223",
   "metadata": {},
   "source": [
    "Естественный язык - изобретение человека, позволяющее с помощью слов описывать различные сущности в реальном или выдуманном мире. Слово (набор символов алфавита) - это знаковый указатель (signifier) на идею или предмет (signified). Например, слово \"ракета\" может означать общую концепцию летательного аппарата на реактивной тяге, так и конкретный экземпляр, который уже произведен на заводе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f7a7f",
   "metadata": {},
   "source": [
    "Неопределенность на уровне выше подразумевает разный смысл предложений в различных контекстах. Например, смысл предложения \"мальчик в клубе склеил модель\" в контексте статьи о пионерах в СССР отличается от рассказа о ночном клубе и успешном похождении мальчика лет 25. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a606c",
   "metadata": {},
   "source": [
    "Неопределенность на уровне ниже (на уровне символов) тоже имеет место быть. В зависимости от контекста, один и тот же символ может интерпретироваться по-разному. Например, если написать символ |3 от руки, возможно интерпретировать его как букву B или число 13.  \n",
    "A  B  C   \n",
    "12 13 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35bcd3",
   "metadata": {},
   "source": [
    "## Основные задачи машинной обработки естественного языка; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e058d8c",
   "metadata": {},
   "source": [
    "В общем случае задача машинной обработки текста на естественном языке - это понимание компьютером смысла написанного. Можно представить себе NLP систему в виде черного ящика, на вход которого подается некоторый текст, который преобразуется в выходной текст, по заранее заданному алгоритму и за конечное время. Под текстом подразумевается любая символьная информация, в том числе числа, векторы, матрицы, элементы множества и т.д.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b3cc4",
   "metadata": {},
   "source": [
    "<img src=\"imgs/NLP_system.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954ead4",
   "metadata": {},
   "source": [
    "В зависимости от сложности решения, сравнения результатов человека и машины, можно разбить задачи NLP на три категории:   \n",
    "\n",
    "Легкие (машина лучше и быстрее человека):\n",
    "- проверка орфографии;\n",
    "- поиск в тексте заданной фразы;\n",
    "- подбор синонимов к слову.\n",
    "\n",
    "Средние (машина быстрее, но не лучше человека):\n",
    "- ответ на вопрос в заданном контексте небольшого документа;\n",
    "- извлечение информации из документов, веб сайтов и т.д.\n",
    "\n",
    "Тяжелые (машина хуже человека):\n",
    "- машинный перевод (например, с Китайского на Английский);\n",
    "- семантический анализ (что означает смысл заданной фразы?);\n",
    "- поиск слов, указывающих на один объект;\n",
    "- ответ на вопрос в общем контексте и т.д.\n",
    "\n",
    "[cs224n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f70e34",
   "metadata": {},
   "source": [
    "## Основные подходы к решению задач: правила, написанные вручную и машинное обучение "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947e29a",
   "metadata": {},
   "source": [
    "Люди воспринимают, понимают и воспроизводят язык с помощью правил, которые закладываются еще в раннем детстве. Лингвистика как наука оперирует с множеством языковых конструкций, используемых в разговорах и письме. \n",
    "За 2000 лет люди добились определенных успехов в выводе правил, которые позволяют работать с хорошо сформулированными высказываниями. Например, базовая английская фраза состоит из необязательного определителя, некоторого количества прилагательных, а затем существительного. \n",
    "\n",
    "С 1960-х по 1985 год в лингвистике и психологии доминировал рационалистический подход, предпогавший что значительная часть знаний в человеческом разуме не извлекается с помощью органов чувств, а фиксируется заранее, предположительно путем генетического наследования. Действительно, сложно представить как дети с раннего возраста способны анализировать слабо структуированные высказывания и выводить из них новые. В частности Хомский постулировал, что ключевые части языка являются врожденными, встроены в мозг при рождении как часть генетической наследственности человека. Ученые пытались создать интеллектуальные системы путем ручного кодирования в них множества исходных знаний и механизмов мышления, имитирующих работу человеческого мозга, но безуспешно. Количество правил стремилось к бесконечности, потому что люди в процессе коммуникации меняют порядок слов, используют идиомы, жаргонные выражения, метафоры и т.д. .\n",
    "\n",
    "Эмпирический подход доминировал между 1920 и 1960 годами, в настоящее время наблюдается его возрождение. Как и рационалистический, этот подход постулирует наличие в мозге некоторых когнитивных способностей. Предполагается, что невозможно обучение с абсолютно чистого листа и существует некоторая исходная структура в мозге, которая определяет способы организации мышления. Мозг ребенка использует ассоциации, распознавание образов и обобщение богатой сенсорной информации, позволяющей создавать ему структуры естественного языка. \n",
    "Эмпирический подход в NLP предполагает, что мы можем изучить сложную и обширную структуру языка, указав соответствующую общую языковую модель, а затем индуцируя значения параметров, применяя статистические методы, методы распознавания образов и машинного обучения к большому корпусу текстов. [Manning, Intro]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bca05",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53923e0b",
   "metadata": {},
   "source": [
    "Установка и настройка рабочей среды. Самый простой вариант для решения практики - это использование Google Colab. Просто загрузите туда ноутбук и можете приступать к выполнению практических заданий. Для запуска python на своем компьютере можете использовать инструкцию из [этого видеоурока](https://www.youtube.com/watch?v=fp5-XQFr_nk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60aac5",
   "metadata": {},
   "source": [
    "Переменные могут хранить различные значения: числа, строки, списки и другие объекты. Имя переменной должно начинаться с буквы: (например *test*)  или знака подчеркивания (например *_test*). Никогда не называйте свои переменные русскими именами на транслите (например *dlina*), всегда используйте английские названия (например *lenght*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e06f6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Целочисленный тип данных int\n",
    "number = 5\n",
    "\n",
    "# Число с плавающей точкой  float\n",
    "fnumber = 5.7\n",
    "\n",
    "# Строковый тип данных str\n",
    "name = 'Andrey'\n",
    "\n",
    "# Булевый или логический тип bool\n",
    "status = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39500b04",
   "metadata": {},
   "source": [
    "C помощью встроенной функции print() можно напечатать что угодно, в том числе значение переменной. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099875f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все что угодно\n",
      "Andrey\n",
      "10.7\n"
     ]
    }
   ],
   "source": [
    "print(\"Все что угодно\")\n",
    "print(name)\n",
    "print(number + fnumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3599f1",
   "metadata": {},
   "source": [
    "Строки могут определяться с помощью одинарных или двойных кавычек. Если нужны кавычки в строке, используйте экранирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0de4429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Это \"сложный\" урок\n"
     ]
    }
   ],
   "source": [
    "print('Это \"сложный\" урок')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06703903",
   "metadata": {},
   "source": [
    "Специальные непечатаемые символы используются для форматирования строк, например '\\n' для перевода строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1e4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных правил, \n",
      "Когда не в шутку занемог...\n"
     ]
    }
   ],
   "source": [
    "print('Мой дядя самых честных правил, \\nКогда не в шутку занемог...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bed105",
   "metadata": {},
   "source": [
    "Для соединения (конкатенации) нескольких строк в одну используется оператор +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e27c29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "hello = 'Hello, ' + 'world!'\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28642847",
   "metadata": {},
   "source": [
    "Для конкатенации строк и чисел испольуется приведение типов с помощью встроенной функции str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b7b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrey 5.7\n"
     ]
    }
   ],
   "source": [
    "print(name + ' ' + str(fnumber))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea183c9",
   "metadata": {},
   "source": [
    "Для ввода информации от пользователя используется встроенная функция input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f9840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите свой возраст: 44\n",
      "Your age is 44 years.\n"
     ]
    }
   ],
   "source": [
    "age = input('Введите свой возраст: ')\n",
    "print('Your age is ' + age + ' years.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf57775",
   "metadata": {},
   "source": [
    "Основные математические операции в python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "328db9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Сложение\n",
    "a = 2\n",
    "b = 3\n",
    "c = a + b\n",
    "print(c)\n",
    "print(b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712f9323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Умножение\n",
    "a = 2\n",
    "b = 3\n",
    "c = a * b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91d7f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Возведение в степень\n",
    "a = 2\n",
    "b = 3\n",
    "c = a ** b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b6c9d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "# Унарный минус\n",
    "a = 2\n",
    "b = - a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2398f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Округление \n",
    "a = 5.65\n",
    "b = round(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c23c57",
   "metadata": {},
   "source": [
    "Для более продвинутых математических операций используется встроенный модуль math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1824e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Округление в большую или меньшую сторону\n",
    "a = 5.65\n",
    "b = math.ceil(a)\n",
    "c = math.floor(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a0a4acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "print(math.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13210f4",
   "metadata": {},
   "source": [
    "Для проверки условий используются операторы if, elif, else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec712995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое число: 2\n",
      "Второе число: 2\n",
      "2 == 2\n"
     ]
    }
   ],
   "source": [
    "a = input('Первое число: ')\n",
    "b = input('Второе число: ')\n",
    "\n",
    "if a > b:\n",
    "    print(a, '>', b)\n",
    "elif a < b:\n",
    "    print(b, '>', a)\n",
    "else:\n",
    "    print(a, '==', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac9fed",
   "metadata": {},
   "source": [
    "Задание 1. Напишите калькулятор, с помошью которого можно сложить или вычесть два числа. Подсказка в видео на 32 минуте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34a21d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a9faf",
   "metadata": {},
   "source": [
    "При написании программы нужно одну большую задачу разбить на несколько более простых. Например, напишем программу, которая считает количество символов во введенном сообщении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55f77119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите сообщение: 3\n",
      "В Вашем сообщении 1 символов.\n"
     ]
    }
   ],
   "source": [
    "text = input('Введите сообщение: ')\n",
    "msg_len = len(text)\n",
    "print('В Вашем сообщении ' + str(msg_len) + ' символов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74ff65",
   "metadata": {},
   "source": [
    "Для того, чтобы выдавать грамматически правильный ответ, напишем свою функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "43cb2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(text):\n",
    "    msg_len = len(text)\n",
    "    answer = 'В Вашем сообщении ' + str(msg_len)\n",
    "    if msg_len == 1:\n",
    "        answer += ' символ.'\n",
    "    elif msg_len > 1 and msg_len < 5:\n",
    "        answer += ' символа.'\n",
    "    elif msg_len >= 5:\n",
    "        answer += ' символов.'\n",
    "    else:\n",
    "        answer += ' ошибка'\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9709047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите сообщение: 4444444\n",
      "В Вашем сообщении 7 символов.\n"
     ]
    }
   ],
   "source": [
    "text = input('Введите сообщение: ')\n",
    "answer = get_answer(text)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f323a8",
   "metadata": {},
   "source": [
    "Функции позволяют делать большое количество работы, скрывая от пользователя всю сложность. Для того, чтобы использовать функции (и классы), которые написали для вас другие программисты, используют модули. Некоторые модули уже включены в стандартную библиотеку (например math), некоторые нужно устанавливать дополнительно. Установка новых модулей с помощью команды **!pip install module_name**  \n",
    "Например, установим модуль для создания собственного телеграмм бота. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38a9ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytelegrambotapi in /opt/anaconda3/envs/deeppavlov/lib/python3.7/site-packages (3.6.7)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/deeppavlov/lib/python3.7/site-packages (from pytelegrambotapi) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/deeppavlov/lib/python3.7/site-packages (from pytelegrambotapi) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/envs/deeppavlov/lib/python3.7/site-packages (from requests->pytelegrambotapi) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/envs/deeppavlov/lib/python3.7/site-packages (from requests->pytelegrambotapi) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/deeppavlov/lib/python3.7/site-packages (from requests->pytelegrambotapi) (2021.10.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/envs/deeppavlov/lib/python3.7/site-packages (from requests->pytelegrambotapi) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytelegrambotapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba7972",
   "metadata": {},
   "source": [
    "Для работы телеграм бота понадобится специальная переменная TOKEN, ее значение можно получить у пользователя с именем @bot_father. Для этого просто перейдите по [ссылке](https://t.me/botfather).  \n",
    "\n",
    "Отправьте @bot_father сообщение /new_bot, придумайте имя для своего бота и получите значение переменной TOKEN. Скопируйте значение в код и запустите ячейку. Можете что-нибудь написать своему боту, он посчитает количество символов в Вашем сообщении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af4a477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "\n",
    "TOKEN = '5441620452:AAGQQtoC8ohgdsvsOUO7ubSD6y86oRQ-hL0'\n",
    "\n",
    "bot = telebot.TeleBot(TOKEN) \n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def send_echo(message):\n",
    "    text = message.text\n",
    "    message_len = len(text)\n",
    "    \n",
    "    answer = 'В вашем сообщении ' + str(message_len) + ' символов.'\n",
    "    \n",
    "    bot.send_message(message.chat.id, answer)\n",
    "\n",
    "bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b842e06",
   "metadata": {},
   "source": [
    "Задание 2. Используйте функцию get_answer(), чтобы бот давал грамматически правильный ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9389010",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = telebot.TeleBot(TOKEN) \n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def send_echo(message):\n",
    "    text = message.text\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    bot.send_message(message.chat.id, answer)\n",
    "\n",
    "bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b9b78",
   "metadata": {},
   "source": [
    "Основы обработки тектов с помощью языка Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d30cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранить текст в файл\n",
    "s = 'Hello world!'\n",
    "with open('hello.txt', 'w') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f42cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# Чтение текста из файла\n",
    "with open('hello.txt') as f:\n",
    "    new_s = f.read()\n",
    "print(new_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b14c60",
   "metadata": {},
   "source": [
    "Модуль string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7b6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd4af3b",
   "metadata": {},
   "source": [
    "Домашнее задание:\n",
    "1. Создайте текстовый файл, который включает в себя несколько строк, например первое четверостишие из \"Евгений Онегин\".\n",
    "2. Прочитайте содержимое файла и выведите его с помощью функции print().\n",
    "3. Замените все строчные буквы на заглавные. \n",
    "4. Сохраните результат в новый файл."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105f6f3",
   "metadata": {},
   "source": [
    "Источники: \n",
    "- https://habr.com/ru/post/481228/\n",
    "- Hobson Lane etc, NLP in action, Питер 2020\n",
    "- Кузнецов О.П. Дискретная математика для инженеров, Лань 2009\n",
    "- Manning Christopher Foundation of statistical NLP\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
