{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef89b4a7",
   "metadata": {},
   "source": [
    "# Основы глубокого обучения "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3638c33",
   "metadata": {},
   "source": [
    "1) Линейный softmax классификатор  \n",
    "2) Обучение линейного классфикатора  \n",
    "3) Многослойный персептрон  \n",
    "4) Рекуррентные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61500133",
   "metadata": {},
   "source": [
    "### 1) Линейный softmax классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aee3ef",
   "metadata": {},
   "source": [
    "Рассмотрим простой линейный классификатор текста, который позволяет определить, к какой из трех тем относится текст (автомобили, мода, компьютеры).  \n",
    "Пусть имеется словарь, содержащий набор $V$ слов естественного языка.  \n",
    "Вектор $X$ - числовое представление текста, в котором каждому индексу соответствует количество вхождений слова из словаря в этом тексте.  \n",
    "$W$ - матрица весов размерностью $(V, C)$, где $V$ - количество слов в словаре, $C$ - количество классов или тем.   \n",
    "В результате умножения вектора $X$ на матрицу $W$ получаем вектор $Z$, в котором каждому элементу соответствует некоторое число. Чем больше число, тем выше вероятность того, что текст принадлежит классу $C$.  \n",
    "Пример линейной классификации текста на три темы представлен на рисунке.    \n",
    "\n",
    "<img src='imgs/linear_class.jpg' width=640>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ceba9",
   "metadata": {},
   "source": [
    "Каждая строка матрицы $W$ умножается на вектор $X$ и результат суммируется. Первая строка отвечает за первый класс, т.е. значения элементов первой строки отражают насколько каждое слово релевантно первому классу. Аналогично со второй и третьей строкой, классов может быть произвольное количество.   \n",
    "Посмотрим внимательно на первую строку. Слова (авто, шина, двигатель) очень похожи на автомобильную тему, поэтому значения там положительные (не обязательно единицы, чем больше, тем выше соответствие). Слово (блок) не однозначно пренадлежит к авто-теме, поэтому там значение w будет ниже. Остальные слова однозначно не соответствуют авто-теме, поэтому там значения w меньше нуля.  \n",
    "Заметим, что в нашем примере слов немного и они сгруппированы в словаре по темам. В реальных задачах размер словаря составляет несколько десятков тысяч слов, часто перемешанных в произвольном порядке. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104442a4",
   "metadata": {},
   "source": [
    "Числа в векторе $Z$ показывают, насколько сильно коррелирует векторное представление текста $X$ с каждой из строк матрицы $W$. Удобно перейти от этих чисел к распределению вероятностей, так чтобы каждый элемент соответствовал вероятности принадлежности текста к заданному классу, а сумма вероятностей равнялась единице.  \n",
    "Для этого используют преобразование softmax(z).  \n",
    "\n",
    "$$\\sigma (z)_{i}={\\frac {e^{z_{i}}}{\\displaystyle \\sum _{k\\mathop {=} 1}^{K}e^{z_{k}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987a8068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.98, 0.  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def softmax(z):\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "softmax([-2, 2, -4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd05ee",
   "metadata": {},
   "source": [
    "### 2) Обучение линейного классфикатора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1e257",
   "metadata": {},
   "source": [
    "Возникает вопрос: как найти значения коэффициентов матрицы $W$? Вручную размечать слова было бы очень утомительно. Автоматический поиск лучших коэффициентов на размеченных данных называется **обучением** классификатора.   \n",
    "Для обучения нам нужно иметь достаточно большой корпус текстов (хотя бы >20), с размеченными классами.  Т.е. каждому вектору $X$ поставить в соответствие вектор $y$ например (0, 1, 0) который показывает пренадлежность ко второму классу.  \n",
    "Имея для каждого $X$ предсказанное распределение вероятностей $s$, мы можем посчитать значение функции потерь (loss), обычно это кросс-энтропия.  \n",
    "\n",
    "$${H} (s,y)=-\\sum _{c}y\\,\\log s(x)$$ \n",
    "\n",
    "С учетом того, что в размеченном векторе $y$ все значения кроме правильного класса равны 0, то кросс-энтропию можно легко посчитать просто взяв логарифм от предсказанной вероятности для правильного класса.  \n",
    "Если классификатор предсказывает правильному классу вероятность близкую к 1, то значение loss будет близко к нулю  \n",
    "$$-log(1) = 0$$\n",
    "Если классификатор предсказывает правильному классу низкую вероятность, например 0.1, то значение loss будет высоким.  \n",
    "$$-log(0.1)=2.7$$\n",
    "Таким образом, нам нужно найти такие значения весов матрицы $W$, при которых loss будет минимальным.  \n",
    "$$loss(X,y,W) \\to min $$\n",
    "Эта оптимизационная задача хорошо решается методом **backpropagation** (обратное распространение ошибки), разработанным в  1974 году независимо и одновременно Галушкиным А.И. и Полом Дж. Вербосом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f749ed",
   "metadata": {},
   "source": [
    "Существует множество методов поиска оптимальных весов:  \n",
    "- случайный перебор (плохая идея);\n",
    "- стохастический градиентный спуск [SGD](https://ru.wikipedia.org/wiki/Стохастический_градиентный_спуск)  \n",
    "- Momentum SGD, RMS, Adam и [другие](https://cs231n.github.io/neural-networks-3/#sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37dbe3",
   "metadata": {},
   "source": [
    "### 3) Многослойный персептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dbdb41",
   "metadata": {},
   "source": [
    "Простой линейный классификатор подходит для несложных задач классификации. Для выделения сложных зависимостей используют комбинацию из линейных и нелинейных функций. \n",
    "\n",
    "<img src='imgs/multilayer.jpg'>  \n",
    "\n",
    "Выход первого слоя - все тот же вектор $z$, является входом для следующего, нелинейного слоя.  \n",
    "Нелинейные слой - это функция активации, которая позволяет лучше решать задачу классификации. На практике часто используют:  \n",
    "- сигмоид;  \n",
    "- гиперболический тангенс;  \n",
    "- ReLU (Rectified Linear Unit) и ее модификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d701471",
   "metadata": {},
   "source": [
    "<img src='imgs/nonlinear.png' width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0f46c",
   "metadata": {},
   "source": [
    "### 4) Рекуррентные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb7dd2",
   "metadata": {},
   "source": [
    "Как мы уже говорили выше, простая модель текста BagOfWords не учитывает порядок слов в предложении. Контекст очень важен для понимания смысла слов, поэтому для повышения качества классификации (и не только) нужен более продвинутый способ обработки текста. Рекуррентные нейросети - один из таких способов. \n",
    "\n",
    "<img src='imgs/rnn.jpg' width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927c889",
   "metadata": {},
   "source": [
    "Каждый токен входного текста представляется в виде вектора $x_i$. Он стыкуется с некоторым начальным значением $h_0$ (обычно нулевым) и подается на вход линейного слоя $W$. Далее на нелинейный слой, опять линейный $H$ и результат в виде вектора $h_{t+1}$ стыкуется со следующим токеном $x_{t+1}$ и все повторяется по кругу. Важно понимать, что веса матриц $W$ и $H$ одинаковы для всех токенов. Обычно архитектуру RNN изображают более компактным способом.\n",
    "\n",
    "<img src='imgs/rnn_.jpg' width=640>\n",
    "\n",
    "Для обозначения конца петли в конце последовательности $X$ добавляют специальный токен $<END>$. Таким образом, в весах матриц $W$ и $H$ сохраняется информация о порядке расположения токенов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab5363",
   "metadata": {},
   "source": [
    "Рекуррентные сети позволяют решать много других задач, например предсказание следующего слова в предложении (языковая модель). Выход $y$ в этом случае будет векторное представление предсказанного слова, той же размерности, что и $x$.  \n",
    "Например:  \n",
    "\n",
    "Было не холодно, дождь шел. Он был одет в **куртку**.  \n",
    "\n",
    "Было холодно, дождь не шел. Он был одет в **шубу**.   \n",
    "\n",
    "В зависимости от порядка слов, вероятность для слов куртка и шуба будет разной.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6025d4",
   "metadata": {},
   "source": [
    "Более продвинутой рекуррентной архитектурой является LSTM (Long Short Term Memory), которая использует отдельные веса для хранения слов, расположенных рядом и расположенных далеко друг от друга.  \n",
    "Например:  \n",
    "\n",
    "Было холодно, дождь не шел. Мне повстречался человек, которого я принял за своего знакомого. Он был одет в **шубу**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55864d1",
   "metadata": {},
   "source": [
    "Реализация и обучение линейного классификатора на numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfbae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
